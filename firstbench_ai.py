# -*- coding: utf-8 -*-
"""firstBench.AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IzYsYyvneE9Ob_JiD6bnNjpONyeWJRNa
"""

import os

# Create directory for raw QuickDraw data if not existing
os.makedirs("quickdraw_raw", exist_ok=True)

# Vocabulary list - target words for drawing recognition
classes = ['apple', 'sun', 'clock', 'basketball', 'donut', 'tree', 'fish', 'cat', 'dog', 'house']

print("Starting download of QuickDraw dataset for target vocabulary...")

# Download .ndjson files for each class
for cls in classes:
    url = f"https://storage.googleapis.com/quickdraw_dataset/full/raw/{cls}.ndjson"
    output = f"quickdraw_raw/{cls}.ndjson"
    try:
        # Download the dataset file
        !wget -q --show-progress -O $output $url
        print(f"Successfully downloaded data for '{cls}'.")
    except Exception as e:
        print(f"Failed to download data for '{cls}': {str(e)}")

print("All dataset downloads complete. Ready for data preprocessing.")

from google.colab import drive
drive.mount('/content/drive')

import zipfile

zip_path = '/content/drive/MyDrive/quickdraw_raw.zip'             # path to your zip file
extract_path = 'quickdraw_raw'             # folder to extract to

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extraction complete.")

import json
import os
import numpy as np
import matplotlib.pyplot as plt

def ndjson_to_images(ndjson_path, output_dir, max_images=1000):
    os.makedirs(output_dir, exist_ok=True)
    with open(ndjson_path, 'r') as f:
        for i, line in enumerate(f):
            if i >= max_images:
                break
            data = json.loads(line)
            strokes = data['drawing']

            plt.figure(figsize=(1.28,1.28))  # 128x128 px size
            plt.axis('off')

            for stroke in strokes:
                x = stroke[0]
                y = stroke[1]
                plt.plot(x, y, 'k-', linewidth=2)

            plt.gca().invert_yaxis()
            plt.savefig(f"{output_dir}/img_{i}.png", bbox_inches='tight', pad_inches=0,dpi=100)
            plt.close()

# Example usage for 'apple' class
ndjson_path = 'quickdraw_raw/apple.ndjson'
output_dir = 'images/apple'
ndjson_to_images(ndjson_path, output_dir, max_images=500)  # You can adjust max_images

for cls in classes:
    print(f"Processing class: {cls}")
    ndjson_path = f'quickdraw_raw/{cls}.ndjson'
    output_dir = f'images/{cls}'
    ndjson_to_images(ndjson_path, output_dir, max_images=500)

# import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
import os
import matplotlib.pyplot as plt
import random

class QuickDrawDataset(Dataset):
    def __init__(self, root_dir, classes, transform=None):
        self.root_dir = root_dir
        self.classes = classes
        self.transform = transform

        self.image_paths = []
        self.labels = []

        for idx, cls in enumerate(classes):
            cls_folder = os.path.join(root_dir, cls)
            for img_name in os.listdir(cls_folder):
                if img_name.endswith('.png'):
                    self.image_paths.append(os.path.join(cls_folder, img_name))
                    self.labels.append(idx)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('L')  # grayscale
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

# Define transforms
transform = transforms.Compose([
    transforms.ToTensor(),
])

# Instantiate dataset
dataset = QuickDrawDataset(root_dir='images', classes=classes, transform=transform)
print(f"Total images in dataset: {len(dataset)}")

# Visualize one random sample per class
classes_to_show = classes[:]
fig, axs = plt.subplots(1, len(classes_to_show), figsize=(15, 3))

for i, cls in enumerate(classes_to_show):
    cls_indices = [idx for idx, label in enumerate(dataset.labels) if label == i]
    sample_idx = random.choice(cls_indices)
    img, label = dataset[sample_idx]
    axs[i].imshow(img.squeeze(), cmap='gray')
    axs[i].set_title(f"{classes[label]}")
    axs[i].axis('off')

plt.show()

from torchvision import transforms
from torch.utils.data import DataLoader

# Define the transform with resize to 28x28
transform = transforms.Compose([
    transforms.Resize((28, 28)),  # Resize images to 28x28
    transforms.Grayscale(),       # If your images are RGB, convert to grayscale (1 channel)
    transforms.ToTensor(),
    # Optional normalization if you used it during training:
    # transforms.Normalize((0.5,), (0.5,))
])

# Assuming your dataset class accepts a transform argument
# Recreate dataset with the updated transform
dataset = QuickDrawDataset(root_dir='images', classes=classes, transform=transform)

# Create DataLoader for training
batch_size = 32
train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)

# Check the first batch shape
images, labels = next(iter(train_loader))
print(f"Batch image tensor shape: {images.shape}")  # Expected: [batch_size, 1, 28, 28]
print(f"Batch labels tensor shape: {labels.shape}")  # Expected: [batch_size]

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split

# Assuming 'dataset' and 'classes' are already defined and loaded

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 28x28 -> 28x28
            nn.ReLU(),
            nn.MaxPool2d(2),                             # 28x28 -> 14x14
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),                             # 14x14 -> 7x7
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# Split dataset into train/test sets
dataset_size = len(dataset)
indices = list(range(dataset_size))
train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42, stratify=dataset.labels)

train_dataset = Subset(dataset, train_indices)
test_dataset = Subset(dataset, test_indices)

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

print(f"Training samples: {len(train_dataset)}, Testing samples: {len(test_dataset)}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = SimpleCNN(num_classes=len(classes)).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Learning rate scheduler (optional)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

def evaluate(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

best_test_acc = 0.0
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        try:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            if batch_idx % 50 == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"Error during training batch {batch_idx}: {e}")

    train_loss = running_loss / total
    train_acc = correct / total

    test_acc = evaluate(model, test_loader, device)

    print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}")

    # Save checkpoint if test accuracy improves
    if test_acc > best_test_acc:
        best_test_acc = test_acc
        torch.save(model.state_dict(), "best_model.pth")
        print(f"Best model saved with Test Acc: {best_test_acc:.4f}")

    # Step the scheduler
    scheduler.step()

print("Training complete.")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Get all predictions
all_preds = []
all_labels = []

model.eval()
with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
disp.plot(cmap='Blues', xticks_rotation='vertical')
plt.title("Confusion Matrix")
plt.show()

import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image

# Define your transform (same as training)
transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

def give_feedback(image_path, model, class_names, device):
    model.eval()
    image = Image.open(image_path).convert('L')  # Grayscale for consistency
    image_tensor = transform(image).unsqueeze(0).to(device)

    with torch.no_grad():
        outputs = model(image_tensor)
        probabilities = F.softmax(outputs, dim=1)[0]
        confidence, predicted_idx = torch.max(probabilities, 0)

    predicted_class = class_names[predicted_idx.item()]
    confidence_value = confidence.item()

    if confidence_value >= 0.8:
        feedback = f"âœ… Great job! That looks like a **{predicted_class}**! (Confidence: {confidence_value:.2f})"
    elif 0.4 <= confidence_value < 0.8:
        feedback = f"ðŸ¤” Could it be a **{predicted_class}**? Try refining your drawing. (Confidence: {confidence_value:.2f})"
    else:
        feedback = f"âŒ Hmm... I'm not sure. Try again! Hint: a **{predicted_class}** usually has distinct features. (Confidence: {confidence_value:.2f})"

    return predicted_class, confidence_value, feedback

import matplotlib.pyplot as plt
from PIL import Image

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

num_images = 5  # number of images per class to test
incorrect_predictions = []

for cls in classes:
    print(f"\nðŸ” Testing class: {cls.upper()}")

    fig, axs = plt.subplots(1, num_images, figsize=(num_images * 2.2, 3))
    fig.suptitle(f"Class: {cls.upper()}", fontsize=14)

    for i in range(num_images):
        image_path = f"/content/images/{cls}/img_{i}.png"

        predicted_class, confidence, feedback = give_feedback(image_path, model, classes,device)
        img = Image.open(image_path)
        axs[i].imshow(img, cmap='gray')
        axs[i].axis('off')

        is_correct = predicted_class == cls
        color = 'green' if is_correct else 'red'

        # Set colored title
        axs[i].set_title(
            f"True: {cls}\nPred: {predicted_class}\nConf: {confidence:.2f}",
            fontsize=9,
            color=color
        )

        # Collect incorrect predictions
        if not is_correct:
            incorrect_predictions.append({
                "actual": cls,
                "predicted": predicted_class,
                "image_path": image_path,
                "confidence": confidence
            })

    plt.tight_layout()
    plt.subplots_adjust(top=0.75)
    plt.show()

# Print easy-to-understand error summary
if incorrect_predictions:
    print("\nâ— Misclassified Images â€“ Explained Simply:")
    for err in incorrect_predictions:
        actual, predicted, path, conf = err["actual"], err["predicted"], err["image_path"], err["confidence"]
        print(f"ðŸ–¼ï¸ {os.path.basename(path)} â€” It was a drawing of a **{actual}**, but the AI thought it's a **{predicted}** with {conf:.0%} confidence.")
        if actual == "sun" and predicted == "cat":
            print("ðŸ‘‰ Likely confused sun rays with whiskers or fur.")
        elif actual == "clock" and predicted == "basketball":
            print("ðŸ‘‰ Both are circles with inner lines; unclear clock hands might look like basketball seams.")
else:
    print("\nðŸŽ‰ Great news! All predictions were correct.")

import streamlit as st
from PIL import Image
import torch
import torchvision.transforms as transforms

# Load your model
model = torch.load('best_model.pth', map_location=torch.device('cpu'))
model.eval()

# Class names
class_names = ['apple', 'basketball', 'cat', 'clock', 'dog', 'fish', 'house', 'sun']  # Example

# Transform
transform = transforms.Compose([
    transforms.Grayscale(),  # if model trained on grayscale
    transforms.Resize((28, 28)),  # Adjust to match your training size
    transforms.ToTensor()
])

# Hint dictionary
hint_suggestions = {
    ("sun", "cat"): "Try adding rays to show it's a sun!",
    ("clock", "basketball"): "Try drawing the hands or numbers!",
    ("fish", "dog"): "Fish usually have fins and tails!",
    ("house", "basketball"): "Add a roof or windows to show it's a house!"
}

# Streamlit app
st.title("ðŸŽ¨ Draw-and-Learn AI Tool")
uploaded_file = st.file_uploader("Upload your drawing", type=["png", "jpg", "jpeg"])

if uploaded_file is not None:
    # Display the image
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="Your drawing", use_column_width=True)

    # Preprocess
    image_tensor = transform(image).unsqueeze(0)  # shape: [1, 1, 28, 28]
    output = model(image_tensor)
    _, predicted = torch.max(output, 1)
    predicted_label = class_names[predicted.item()]

    # Optional true label (hardcoded or from metadata, file name, etc.)
    true_label = st.selectbox("Choose correct label for testing hints", class_names)

    st.subheader(f"ðŸ” Predicted: **{predicted_label}**")

    # HINT system
    if predicted_label != true_label:
        hint = hint_suggestions.get((true_label, predicted_label), "Try adding more details!")
        st.warning(f"ðŸ¤– Hint: {hint}")
    else:
        st.success("âœ… Great job! Your drawing is recognized correctly.")

